{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU memory setting failed: {e}\")"
      ],
      "metadata": {
        "id": "fMju6bm4pYzg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fcc964b"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29f078dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8dc7942-8087-4f98-ef24-3ec68cf09124"
      },
      "source": [
        "# load dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    data = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "train_data = load_dataset(\"/content/drive/MyDrive/ALU/datasets/train_data.jsonl\")\n",
        "test_data = load_dataset(\"/content/drive/MyDrive/ALU/datasets/test_data.jsonl\")\n",
        "all_data = train_data + test_data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76e6f153"
      },
      "source": [
        "# Extract conversation pairs\n",
        "def extract_pairs(convo):\n",
        "    messages = convo[\"messages\"]\n",
        "    return [\n",
        "        (\" \".join(msg[\"text\"] for msg in messages[:i]), messages[i][\"text\"])\n",
        "        for i in range(1, len(messages))\n",
        "    ]\n",
        "\n",
        "pairs = []\n",
        "for convo in all_data:\n",
        "    pairs.extend(extract_pairs(convo))\n",
        "\n",
        "if not pairs:\n",
        "    print(\"Warning: No messages found. Using alternative extraction method\")\n",
        "    for convo in all_data:\n",
        "        if \"conversation\" in convo:\n",
        "            messages = convo[\"conversation\"]\n",
        "            pairs.extend(\n",
        "                [\n",
        "                    (\" \".join(messages[:i]), messages[i])\n",
        "                    for i in range(1, len(messages))\n",
        "                ]\n",
        "            )\n",
        "\n",
        "df = pd.DataFrame(pairs, columns=[\"context\", \"response\"])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13e85c89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9363e246-1d38-42fa-c74a-60e71110762f"
      },
      "source": [
        "# preprocessing\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_data(row):\n",
        "    return tokenizer(\n",
        "        f\"CONTEXT: {row['context']} RESPONSE: {row['response']}<|endoftext|>\",\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "tokenized_data = df.apply(tokenize_data, axis=1)\n",
        "input_ids = [x[\"input_ids\"] for x in tokenized_data]\n",
        "attention_mask = [x[\"attention_mask\"] for x in tokenized_data]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b2e176d"
      },
      "source": [
        "# Create TensorFlow dataset\n",
        "dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": input_ids,\n",
        "        }\n",
        "    )\n",
        "    .batch(8)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12c67b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf7fc41-52d9-4512-887e-86814ab0e6e2"
      },
      "source": [
        "# Save processed data\n",
        "tf.data.experimental.save(dataset, \"processed_data\")\n",
        "df.to_csv(\"conversation_pairs.csv\", index=False)\n",
        "print(\"Preprocessing complete! Processed\", len(df), \"conversation pairs\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tmp/ipython-input-6-3317166382.py:2: save (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.save(...)` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete! Processed 194754 conversation pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\n",
        "    \"gpt2\", pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Load processed data\n",
        "dataset = tf.data.experimental.load(\n",
        "    \"processed_data\",\n",
        "    element_spec={\n",
        "        \"input_ids\": tf.TensorSpec(shape=(None, 128), dtype=tf.int32),\n",
        "        \"attention_mask\": tf.TensorSpec(shape=(None, 128), dtype=tf.int32),\n",
        "        \"labels\": tf.TensorSpec(shape=(None, 128), dtype=tf.int32),\n",
        "    },\n",
        ")\n"
      ],
      "metadata": {
        "id": "4Yk8txHC66l1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02150e89-b06c-44b8-f7cf-7725517ce393"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "WARNING:tensorflow:From /tmp/ipython-input-7-3395717798.py:14: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.load(...)` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FREEZE MOST LAYERS - Only train the last 3 layers\n",
        "print(\"Freezing lower layers for faster training...\")\n",
        "for i, layer in enumerate(model.layers):\n",
        "    if i < len(model.layers) - 3:  # Freeze all but last 3 layers\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True\n",
        "\n",
        "# Count trainable parameters\n",
        "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
        "total_params = sum([tf.keras.backend.count_params(w) for w in model.weights])\n",
        "print(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n",
        "\n",
        "# REMOVED MIXED PRECISION - CAUSES MEMORY ISSUES\n",
        "# Enable mixed precision for memory reduction and speed up (requires compatible hardware like GPU/TPU)\n",
        "# from tensorflow.keras import mixed_precision\n",
        "# mixed_precision.set_global_policy('mixed_float16')\n",
        "# print(\"Mixed precision enabled.\")\n",
        "\n",
        "# Configure training - ULTRA OPTIMIZED\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=3e-4,  # Slightly reduced from 5e-4\n",
        "    epsilon=1e-7,\n",
        "    clipnorm=0.5\n",
        ")\n",
        "\n",
        "# Simplified loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Compile with XLA for speed\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    run_eagerly=False,\n",
        "    jit_compile=True  # Enable XLA compilation for speed boost\n",
        ")\n",
        "\n",
        "# ULTRA-OPTIMIZED PARAMETERS\n",
        "dataset_size = 256\n",
        "batch_size = 16      # REDUCED from 64 to prevent OOM\n",
        "max_length = 64      # REDUCED from 32 to prevent memory issues\n",
        "epochs = 6\n",
        "\n",
        "steps_per_epoch = dataset_size // batch_size\n",
        "print(f\"Optimized steps per epoch: {steps_per_epoch}\")\n",
        "\n",
        "# Estimate total steps\n",
        "total_steps = steps_per_epoch * epochs\n",
        "print(f\"Total steps for {epochs} epochs: {total_steps}\")\n",
        "\n",
        "# Create smaller subset for ultra-fast training\n",
        "df_subset = df.sample(n=min(dataset_size, len(df)), random_state=42)\n",
        "print(f\"Using {len(df_subset)} samples instead of {len(df)}\")\n",
        "\n",
        "# Ensure padding token is set for the tokenizer within this scope\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_data_optimized(row):\n",
        "    return tokenizer(\n",
        "        f\"CONTEXT: {row['context']} RESPONSE: {row['response']}<|endoftext|>\",\n",
        "        max_length=max_length, # Use the reduced max_length\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "tokenized_data_subset = df_subset.apply(tokenize_data_optimized, axis=1)\n",
        "input_ids_subset = [x[\"input_ids\"] for x in tokenized_data_subset]\n",
        "attention_mask_subset = [x[\"attention_mask\"] for x in tokenized_data_subset]\n",
        "\n",
        "dataset_subset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        {\n",
        "            \"input_ids\": input_ids_subset,\n",
        "            \"attention_mask\": attention_mask_subset,\n",
        "            \"labels\": input_ids_subset,\n",
        "        }\n",
        "    )\n",
        "    .batch(batch_size)\n",
        "    .cache()  # Cache in memory\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Minimal callbacks for speed and early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='loss',\n",
        "        patience=2,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "]\n",
        "\n",
        "# Track training time\n",
        "import time\n",
        "print(\"Starting ultra-optimized training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Ultra-fast training\n",
        "history = model.fit(\n",
        "    dataset_subset,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False\n",
        ")\n",
        "\n",
        "# Calculate actual training time\n",
        "end_time = time.time()\n",
        "actual_time = (end_time - start_time) / 60\n",
        "print(f\"\\nActual training time: {actual_time:.1f} minutes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwSQROGtplrH",
        "outputId": "4c7eaa25-546c-420a-875b-ddb202c22fca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing lower layers for faster training...\n",
            "Trainable parameters: 124,439,808 / 124,439,808\n",
            "Optimized steps per epoch: 16\n",
            "Total steps for 6 epochs: 96\n",
            "Using 256 samples instead of 194754\n",
            "Starting ultra-optimized training...\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7c35cb006700> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7c35cb006700> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "16/16 [==============================] - 439s 21s/step - loss: 1.3583\n",
            "Epoch 2/6\n",
            "16/16 [==============================] - 336s 21s/step - loss: 0.0439\n",
            "Epoch 3/6\n",
            "16/16 [==============================] - 341s 21s/step - loss: 0.0271\n",
            "Epoch 4/6\n",
            "16/16 [==============================] - 337s 21s/step - loss: 0.0183\n",
            "Epoch 5/6\n",
            "16/16 [==============================] - 338s 21s/step - loss: 0.0152\n",
            "Epoch 6/6\n",
            "16/16 [==============================] - 335s 21s/step - loss: 0.0103\n",
            "\n",
            "Actual training time: 36.4 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"sidekick_model\")\n",
        "tokenizer.save_pretrained(\"sidekick_model\")\n",
        "print(\"Training complete! Model saved\")"
      ],
      "metadata": {
        "id": "-5vDNiuV8wcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d172cf-d300-4677-ce40-41efcc11835d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete! Model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yoQOJjmIQkud"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}