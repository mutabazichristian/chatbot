{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU memory setting failed: {e}\")"
      ],
      "metadata": {
        "id": "fMju6bm4pYzg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fcc964b"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29f078dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8dc7942-8087-4f98-ef24-3ec68cf09124"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    data = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "train_data = load_dataset(\"/content/drive/MyDrive/ALU/datasets/train_data.jsonl\")\n",
        "test_data = load_dataset(\"/content/drive/MyDrive/ALU/datasets/test_data.jsonl\")\n",
        "all_data = train_data + test_data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76e6f153"
      },
      "source": [
        "def extract_pairs(convo):\n",
        "    messages = convo[\"messages\"]\n",
        "    return [\n",
        "        (\" \".join(msg[\"text\"] for msg in messages[:i]), messages[i][\"text\"])\n",
        "        for i in range(1, len(messages))\n",
        "    ]\n",
        "\n",
        "pairs = []\n",
        "for convo in all_data:\n",
        "    pairs.extend(extract_pairs(convo))\n",
        "\n",
        "if not pairs:\n",
        "    print(\"Warning: No messages found. Using alternative extraction method\")\n",
        "    for convo in all_data:\n",
        "        if \"conversation\" in convo:\n",
        "            messages = convo[\"conversation\"]\n",
        "            pairs.extend(\n",
        "                [\n",
        "                    (\" \".join(messages[:i]), messages[i])\n",
        "                    for i in range(1, len(messages))\n",
        "                ]\n",
        "            )\n",
        "\n",
        "df = pd.DataFrame(pairs, columns=[\"context\", \"response\"])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13e85c89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9363e246-1d38-42fa-c74a-60e71110762f"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_data(row):\n",
        "    return tokenizer(\n",
        "        f\"CONTEXT: {row['context']} RESPONSE: {row['response']}<|endoftext|>\",\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "tokenized_data = df.apply(tokenize_data, axis=1)\n",
        "input_ids = [x[\"input_ids\"] for x in tokenized_data]\n",
        "attention_mask = [x[\"attention_mask\"] for x in tokenized_data]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b2e176d"
      },
      "source": [
        "dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": input_ids,\n",
        "        }\n",
        "    )\n",
        "    .batch(8)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12c67b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf7fc41-52d9-4512-887e-86814ab0e6e2"
      },
      "source": [
        "tf.data.experimental.save(dataset, \"processed_data\")\n",
        "df.to_csv(\"conversation_pairs.csv\", index=False)\n",
        "print(\"Preprocessing complete! Processed\", len(df), \"conversation pairs\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tmp/ipython-input-6-3317166382.py:2: save (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.save(...)` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete! Processed 194754 conversation pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\n",
        "    \"gpt2\", pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "dataset = tf.data.experimental.load(\n",
        "    \"processed_data\",\n",
        "    element_spec={\n",
        "        \"input_ids\": tf.TensorSpec(shape=(None, 128), dtype=tf.int32),\n",
        "        \"attention_mask\": tf.TensorSpec(shape=(None, 128), dtype=tf.int32),\n",
        "        \"labels\": tf.TensorSpec(shape=(None, 128), dtype=tf.int32),\n",
        "    },\n",
        ")\n"
      ],
      "metadata": {
        "id": "4Yk8txHC66l1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02150e89-b06c-44b8-f7cf-7725517ce393"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "WARNING:tensorflow:From /tmp/ipython-input-7-3395717798.py:14: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.load(...)` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Freezing lower layers for faster training...\")\n",
        "for i, layer in enumerate(model.layers):\n",
        "    if i < len(model.layers) - 3:\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True\n",
        "\n",
        "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
        "total_params = sum([tf.keras.backend.count_params(w) for w in model.weights])\n",
        "print(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=3e-4,\n",
        "    epsilon=1e-7,\n",
        "    clipnorm=0.5\n",
        ")\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    run_eagerly=False,\n",
        "    jit_compile=True\n",
        ")\n",
        "\n",
        "dataset_size = 256\n",
        "batch_size = 16\n",
        "max_length = 64\n",
        "epochs = 6\n",
        "\n",
        "steps_per_epoch = dataset_size // batch_size\n",
        "print(f\"Optimized steps per epoch: {steps_per_epoch}\")\n",
        "\n",
        "total_steps = steps_per_epoch * epochs\n",
        "print(f\"Total steps for {epochs} epochs: {total_steps}\")\n",
        "\n",
        "df_subset = df.sample(n=min(dataset_size, len(df)), random_state=42)\n",
        "print(f\"Using {len(df_subset)} samples instead of {len(df)}\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_data_optimized(row):\n",
        "    return tokenizer(\n",
        "        f\"CONTEXT: {row['context']} RESPONSE: {row['response']}<|endoftext|>\",\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "tokenized_data_subset = df_subset.apply(tokenize_data_optimized, axis=1)\n",
        "input_ids_subset = [x[\"input_ids\"] for x in tokenized_data_subset]\n",
        "attention_mask_subset = [x[\"attention_mask\"] for x in tokenized_data_subset]\n",
        "\n",
        "dataset_subset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        {\n",
        "            \"input_ids\": input_ids_subset,\n",
        "            \"attention_mask\": attention_mask_subset,\n",
        "            \"labels\": input_ids_subset,\n",
        "        }\n",
        "    )\n",
        "    .batch(batch_size)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='loss',\n",
        "        patience=2,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "]\n",
        "\n",
        "import time\n",
        "print(\"Starting ultra-optimized training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_subset,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "actual_time = (end_time - start_time) / 60\n",
        "print(f\"\\nActual training time: {actual_time:.1f} minutes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwSQROGtplrH",
        "outputId": "4c7eaa25-546c-420a-875b-ddb202c22fca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing lower layers for faster training...\n",
            "Trainable parameters: 124,439,808 / 124,439,808\n",
            "Optimized steps per epoch: 16\n",
            "Total steps for 6 epochs: 96\n",
            "Using 256 samples instead of 194754\n",
            "Starting ultra-optimized training...\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7c35cb006700> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7c35cb006700> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "16/16 [==============================] - 439s 21s/step - loss: 1.3583\n",
            "Epoch 2/6\n",
            "16/16 [==============================] - 336s 21s/step - loss: 0.0439\n",
            "Epoch 3/6\n",
            "16/16 [==============================] - 341s 21s/step - loss: 0.0271\n",
            "Epoch 4/6\n",
            "16/16 [==============================] - 337s 21s/step - loss: 0.0183\n",
            "Epoch 5/6\n",
            "16/16 [==============================] - 338s 21s/step - loss: 0.0152\n",
            "Epoch 6/6\n",
            "16/16 [==============================] - 335s 21s/step - loss: 0.0103\n",
            "\n",
            "Actual training time: 36.4 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"sidekick_model\")\n",
        "tokenizer.save_pretrained(\"sidekick_model\")\n",
        "print(\"Training complete! Model saved\")"
      ],
      "metadata": {
        "id": "-5vDNiuV8wcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d172cf-d300-4677-ce40-41efcc11835d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete! Model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yoQOJjmIQkud"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
